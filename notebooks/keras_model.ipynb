{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 75\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "from btsc_dataset import load_full_dataset, split_images_and_labels, resize_images\n",
    "from utils import train_validation_test_split\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of increasing image size from 32->64: No accuracy change. Significantly longer train times.\n",
    "# Effect of increasing image size from 32->16: ~3% accuracy decrease. Significantly faster train times.\n",
    "# from sklearn.preprocessing import normalize\n",
    "\n",
    "resized_data = resize_images(load_full_dataset(), size=32)\n",
    "(images, labels) = split_images_and_labels(resized_data)\n",
    "unique_labels = set(labels)\n",
    "\n",
    "print('Images:', images.shape)\n",
    "print('Labels:', labels.shape)\n",
    "print('Total Unique Labels:', len(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# [print(x[0][0].shape) for x in images]\n",
    "\n",
    "def normalize(img):\n",
    "    new_img = img.copy()\n",
    "    \n",
    "    num_channels = 3\n",
    "    for channel in range(num_channels):\n",
    "        total = float(0)\n",
    "        \n",
    "        for row in img:\n",
    "            for pixel in row:\n",
    "                total += float(pixel[channel])\n",
    "\n",
    "        height  = len(img)\n",
    "        width   = len(img[0])\n",
    "        average = total / (width * height)\n",
    "\n",
    "        for row_idx in range(height):\n",
    "            for pixel_idx in range(width):\n",
    "#                 print('GOT:', img[row_idx][pixel_idx])\n",
    "                new_img[row_idx][pixel_idx][channel] = img[row_idx][pixel_idx][channel] // average\n",
    "    \n",
    "    return new_img\n",
    "\n",
    "def do_normalize(img, idx):\n",
    "#     print('Normalizing:', idx)\n",
    "    return normalize(img)\n",
    "    \n",
    "images_2 = np.array([do_normalize(img, idx) for (idx, img) in enumerate(images)])\n",
    "# images_2 = np.array([normalize(x) for x in images])\n",
    "# images_2\n",
    "print('DONE:', images_2.shape)\n",
    "print(images_2[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello?...')\n",
    "def plot_images_and_labels(images_np, labels_np):\n",
    "    images = images_np.tolist()\n",
    "    labels = labels_np.tolist()\n",
    "    \n",
    "    def first_image_with_label(label):\n",
    "        x = images[labels.index(label)]\n",
    "        print('X:', images)\n",
    "        return x\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    print('ABOVE')\n",
    "    unique_labels = sorted(set(labels))\n",
    "    for (idx, label) in enumerate(unique_labels, 1):\n",
    "        print('WITHIN:', idx, label)\n",
    "        plt.subplot(8, 8, idx)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'{label} ({labels.count(label)})')\n",
    "        plt.imshow(first_image_with_label(label))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_images_and_labels(images_2, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "integer_labels= LabelEncoder().fit_transform(labels)\n",
    "ont_hot_labels = to_categorical(integer_labels)\n",
    "\n",
    "X = images\n",
    "y = ont_hot_labels\n",
    "\n",
    "X_train, X_valid, X_test, \\\n",
    "y_train, y_valid, y_test = train_validation_test_split(X, y, random_state=42)\n",
    "\n",
    "print('X:', X.shape)\n",
    "print('y:', y.shape)\n",
    "print()\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print()\n",
    "print('X_valid:', X_valid.shape)\n",
    "print('y_valid:', y_valid.shape)\n",
    "print()\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Effect of increasing filters from 16 to 32: ~1% accuracy decrease\n",
    "# Effect of increasing filters from 16 to 64: ~1% accuracy increase\n",
    "model.add(Conv2D(64, (3, 3), input_shape=X[0].shape, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Effect of adding this layer: no accuracy change\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# Effect of adding this layer: No accuracy change (~95-96%)\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "# Effect of increasing nodes from 64->128: No accuracy change (~95-96%).\n",
    "# Effect of decreasing nodes from 64->32: ~10% accuracy decrease (~85%).\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(unique_labels), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=50, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracies per epoch\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation losses per epoch\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "# print('Test Results:')\n",
    "# print('  Loss     =', loss)\n",
    "# print('  Accuracy =', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
